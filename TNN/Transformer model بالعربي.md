#### _تلخيص الورقة ("الانتباه هو كل ما تحتاجه"):

تقدم الورقة المعنونة "الانتباه هو كل ما تحتاجه" بنية شبكية جديدة تسمى "التحول"، والتي تعتمد فقط على آليات الانتباه وتقضي على الحاجة إلى الشبكات العصبية التكرارية أو التحويلية. يقترح الكاتبون هذه البنية كبديل لنماذج نقل التسلسل الحالية. تظهر نموذج التحول جودة متفوقة، وقابلية للتوازي، وتقليل في وقت التدريب بالمقارنة مع النماذج التقليدية.

تبدأ الورقة بمناقشة بروز الشبكات العصبية التكرارية (RNNs) في نمذجة التسلسل ومهام نقل التسلسل مثل نمذجة اللغة وترجمة الآلة. ومع ذلك، تعاني RNNs من الحوسبة التسلسلية والتوازي المحدود، خاصةً مع تسلسلات أطول. تم استخدام آليات الانتباه بالتزامن مع RNNs لنمذجة التبعيات بين المواقع في التسلسل. يقترح الكاتبون نموذج التحول كبنية تعتمد فقط على آليات الانتباه، مما يسمح بزيادة التوازي وتحسين الأداء.

تقدم الورقة هندسة نموذج التحول ومكوناته، بما في ذلك كتلة المشفر وفك الشفر. يتألف المشفر من عدة طبقات مع آليات الانتباه الذاتي وشبكات متصلة بالمواقع. يدمج فك الشفر، بالإضافة إلى طبقات المشفر، انتباه رأسي متعدد فوق إخراج المشفر. يتم وصف وظيفة الانتباه المستخدمة في التحول على أنها انتباه نقطي مقسم، الذي يحسب التوافق بين الاستعلامات والمفاتيح للحصول على قيم مزوّدة بالأوزان.

يسلط الكاتبون الضوء على مزايا نموذج التحول على النهج الحالي، مثل تقليل التعقيد الحسابي وزيادة التوازي. يجرون تجارب على مهام ترجمة الآلة، حيث يظهر التحول أداءً متفوقًا على النماذج السابقة من حيث جودة الترجمة. يحقق نموذج التحول نتائج على مستوى الفن في مهام ترجمة الإنجليزية إلى الألمانية والفرنسية في WMT 2014 مع الحاجة إلى وقت تدريب أقل بكثير.

وعلاوة على ذلك، يظهر الكاتبون أن نموذج التحول يتعمق في مهام أخرى بنجاح عن طريق تطبيقه على تحليل الجمل الإنجليزية باستخدام بيانات تدريب كبيرة ومحدودة. يقدمون وصفاً مفصلاً لهندسة النموذج وعملية التدريب ومقاييس التقييم.

في الختام، تقدم الورقة نموذج التحول كبنية جديدة لمهام نقل التسلسل. من خلال استغلال آليات الانتباه والتخلص من الاعتماد على الشبكات التكرارية أو التحويلية، يحقق التحول أداءً محسنًا وقابلية للتوازي وكفاءة في التدريب. تؤكد النتائج التجريبية فعالية التحول في مهام ترجمة الآلة، وتحقيق نتائج على مستوى الفن الحالي.

---

## الشرح:

الشبكة العصبية "التحول" هي نموذج قوي في مجال التعلم العميق تم تقديمه في ورقة بحثية هامة بعنوان "[[الانتباه هو كل ما تحتاجه..pdf]]" من قبل فاستواني وآخرين في عام 2017. ثورت هذه التقنية مجال معالجة اللغة الطبيعية ومن ثم وجدت تطبيقات في مجالات متنوعة. تعتمد بنية التحول على مفهوم الانتباه، مما يمكّنها من التقاط التبعيات على المدى الطويل وتحقيق أداء رائد على مجموعة واسعة من المهام.

تطبيقات شبكة التحول العصبية:

1. **ترجمة الآلة**: حققت التحول نتائج مذهلة في مهام ترجمة الآلة، مثل ترجمة النصوص من لغة إلى أخرى. قدرتها على التقاط التبعيات على المدى الطويل والتعامل مع تسلسلات الإدخال ذات الطول المتغير يجعلها مناسبة تمامًا لهذه المهمة.
    
2. **إنشاء النصوص**: يمكن استخدام التحول لإنتاج نصوص متناسقة وذات سياق. تم تطبيقها في مهام مثل إنشاء مقالات الأخبار، وأنظمة الحوار، وإنشاء القصص.
    
3. **تلخيص وفهم الوثائق**: يمكن لآلية الانتباه في التحول التركيز على أجزاء مهمة من وثيقة أو نص، مما يجعلها فعّالة لمهام مثل تلخيص النصوص، وتصنيف الوثائق، وتحليل المشاعر.
    
4. **التعرف على الكلام**: تم تطبيق التحول أيضًا في مهام التعرف التلقائي على الكلام، حيث يحول الكلام المنطوق إلى نص مكتوب. أظهرت نتائج واعدة في هذا المجال أيضًا.
    
5. **الإجابة على الأسئلة**: قدرة التحول على فهم وإنتاج النص جعلتها مفيدة في أنظمة الإجابة على الأسئلة. يمكنها معالجة سؤال وفقرة سياق وإنتاج إجابات ذات صلة.
    
6. **التعرف على الصور**: بالرغم من تصميمها في المقام الأول لمهام معالجة اللغة، وجدت التحول أيضًا تطبيقات في مهام الرؤية الحاسوبية. يمكن تكييفها لمهام التعرف على الصور عن طريق التعامل مع الصور كتسلسلات من الباتشات.

---

قبل ظهور شبكات التحول العصبية (TNNs)، كانت الشبكات العصبية التكرارية (RNNs) تستخدم عادة لمهام المعالجة التسلسلية، بما في ذلك ترجمة الآلة. ومع ذلك، كانت RNNs مميزة بسرعة معالجة بطيئة، ودقة محدودة، وتحديات في التعامل مع مجموعات البيانات الكبيرة.

**_هنا كيف تعمل RNN:_** تم تصميمها لمعالجة البيانات التسلسلية، حيث يعتمد الإدخال الحالي ليس فقط على الحالة الحالية ولكن أيضًا على الإدخالات والحالات السابقة.

![[Pasted image 20231220154637.png]]
فلنفترض أن لدينا هذه الجملة "I work at the university." ونريد ترجمتها إلى العربية "أنا أعمل في الجامعة."

في مهمة الترجمة، يقوم الشبكة العصبية التكرارية (RNN) بتحليل كل كلمة ('I', 'work', 'at', 'the', 'university') تلو الأخرى، محدثة الحالة الخفية في كل خطوة. يتأثر الإخراج في كل خطوة بالكلمة الحالية والحالة الخفية، التي تلتقط المعلومات التاريخية من الكلمات السابقة. الإخراج النهائي هو سلسلة من الكلمات المترجمة ('أنا', 'أعمل', 'في', 'الجامعة') باللغة العربية.

![[Pasted image 20240107144003.png]]
#### المشاكل مع الشبكات العصبية التكرارية (RNN):

1. بطء الحوسبة لتسلسلات طويلة
2. اختفاء أو انفجار التدرجات
3. صعوبة في الوصول إلى المعلومات منذ وقت طويل

فعلاً، تميل الشبكات العصبية التكرارية إلى أن تكون بطيئة ويمكن أن تواجه صعوبات في التعامل مع مجموعات البيانات الكبيرة، مما قد يؤدي إلى الارتباك المحتمل أو الصعوبات في معالجة البيانات الشاملة. ومع ذلك،

قدمت الشبكة العصبية التحول (TNN) حلاً مبتكرًا يسمى "الانتباه الذاتي" في ورقة البحث "الانتباه هو كل ما تحتاجه." هذا الابتكار تناول هذه المشاكل وفتح الباب أمام تطورات لاحقة مثل GPT، Bert، LLama، انتشار مستقر، وغيرها.

---

## في هذا القسم، سأتناول تفاصيل شبكة التحول العصبية كما ورد في الورقة:

> تعتمد نماذج تحويل التسلسل السائدة على شبكات عصبية متكررة أو تحويلية معقدة تتضمن مُشفّرًا وفك شفرة. تتصل النماذج الأفضل أداءً أيضًا بين المُشفّر وفك الشفرة من خلال آلية الانتباه. نقترح هنا بنية شبكية جديدة بسيطة، وهي التحول، تعتمد حصرًا على آليات الانتباه، دون الحاجة لتكرار الحوسبة والتحويل بشكل كامل. تظهر التجارب على مهمتي ترجمة الآلة تفوق هذه النماذج من حيث الجودة، بينما تكون قابلة للتوازي بشكل أكبر وتتطلب وقتًا أقل بشكل كبير للتدريب. يحقق نموذجنا درجة BLEU تبلغ 28.4 في مهمة ترجمة الإنجليزية إلى الألمانية في WMT 2014، محسنًا على النتائج الأفضل الموجودة حاليًا، بما في ذلك الانسمبل، بمقدار أكثر من 2 نقطة BLEU. في مهمة ترجمة الإنجليزية إلى الفرنسية في WMT 2014، يحقق نموذجنا درجة BLEU الجديدة للفردية بلغت 41.8 بعد التدريب لمدة 3.5 يومًا على ثمانية وحدات معالجة الرسومات (GPUs)، وهو جزء صغير من تكاليف التدريب لأفضل النماذج الموجودة في الأدبيات. نظهر أن نموذج التحول يتعمق جيدًا في المهام الأخرى عند تطبيقه بنجاح على تحليل تركيب الجمل الإنجليزية باستخدام بيانات تدريب كبيرة ومحدودة.

> 1 مقدمة الشبكات العصبية التكرارية، وخاصة الذاكرة القصيرة الطويلة [13] والشبكات العصبية التكرارية المؤرقة [7]، قد ثبتت بشكل قوي كطرق رائدة في نمذجة التسلسل ومشاكل نقل التسلسل مثل نمذجة اللغة وترجمة الآلة [35، 2، 5]. استمرت الجهود العديدة منذ ذلك الحين في دفع حدود نماذج اللغة التكرارية وهندسات التشفير والفك [38، 24، 15]. تفترض النماذج التكرارية حسابًا على طول المواقع الرمزية في تسلسلات الإدخال والإخراج. عند تحديد المواقع إلى خطوات في وقت الحوسبة، يتم إنشاء سلسلة من الحالات الخفية ht، كدالة من الحالة الخفية السابقة ht-1 والإدخال للموقع t. يحول هذا الطابع التسلسلي بطبيعته التوازي داخل أمثلة التدريب، والتي تصبح حاسمة في حالة طول تسلسل أطول، حيث تحد من الذاكرة الدفعية عبر الأمثلة. حققت الأعمال الحديثة تحسينات كبيرة في كفاءة الحوسبة من خلال حيل التجزئة [21] والحوسبة الشرطية [32]، مع تحسين أداء النموذج أيضًا في حالة الحوسبة الشرطية. ومع ذلك، يظل القيد الأساسي للحوسبة التسلسلية قائمًا. أصبحت آليات الانتباه جزءًا لا يتجزأ من نماذج ناجحة لنقل التسلسل ونمذجة التسلسل في مهام متنوعة، مما يتيح نمذجة التبعيات بغض النظر عن مدى انتشارها في تسلسلات الإدخال أو الإخراج [2، 19]. ومع ذلك، في معظم الحالات [27]، يتم استخدام مثل هذه الآليات في الانتباه بالتزامن مع شبكة تكرارية. في هذا العمل، نقترح النموذج التحول، وهو بنية تخلو عن التكرار وتعتمد تمامًا على آلية الانتباه لرسم تبعيات عالمية بين الإدخال والإخراج. يتيح التحول توازيًا بشكل أكبر بكثير ويمكن أن يصل إلى حالة جديدة من الفن في جودة الترجمة بعد تدريبه لمدة تصل إلى اثنتي عشرة ساعة فقط على ثمانية وحدات معالجة الرسومات P100.

### بنية النموذج:

> معظم النماذج التنافسية لنقل التسلسل العصبي لديها هيكل مُشفّر-فك الشفرة [5، 2، 35]. هنا، يقوم المشفر بتصوير تسلسل الرموز التمثيلية (x1، ...، xn) إلى تسلسل من التمثيلات المستمرة z = (z1، ...، zn). مع التمثيل z، يقوم فك الشفرة بإنشاء تسلسل الإخراج (y1، ...، ym) من الرموز عنصرًا واحدًا في كل مرة. في كل خطوة، يكون النموذج تلقائيًا تكراريًا [10]، حيث يستهلك الرموز التي تم إنشاءها سابقًا كإدخال إضافي عند إنشاء الرمز التالي.

![[Pasted image 20231220223916.png]]
  
الرجاء التحقق من هذه اللوحة [[نموذج التحول..canvas|نموذج التحول]] لمراقبة كيفية عمل الكلمات الخاصة، ثم تابع بقراءة القسم التالي لفهم كيفية عمل النموذج نفسه.

لذا، أولاً لدينا الهيكل الأيسر الذي يُعرف بـ "المُشفّر" والهيكل الأيمن الذي يُعرف بـ "فك الشفرة":

1. **Input Embeddings:**  
    تتم تحويل التسلسل الداخلي إلى تضمينات ذات أبعاد ثابتة، تتألف عادة من تضمينات الكلمات وتشفيرات المواقع. تأخذ تضمينات الكلمات على عاتقها تقديم المعنى الدلالي لكل كلمة.
     ![[Pasted image 20240107144248.png]]
   
2. بينما **positional encodings** توضح موجات السين والكوسين موقع الكلمة في التسلسل.
      ![[Pasted image 20240107144329.png]]
    $$P E(pos,2i) = sin(pos/100002i/dmodel )$$
     $$P E(pos,2i+1) = cos(pos/100002i/dmodel )$$
     ![[Pasted image 20240107144405.png]]

### لماذا الدوال الثلاثية؟

الدوال الثلاثية مثل cos وsin تمثل بشكل طبيعي نمطًا يمكن للنموذج التعرف عليه كمستمر، لذلك يصبح من الأسهل رؤية الأوضاع النسبية بالنسبة للنموذج. من خلال مراقبة رسم هذه الدوال، يمكننا أيضًا رؤية نمط منتظم، لذلك يمكننا أن نفترض أن النموذج سيلاحظ ذلك أيضًا.
     
3. **Encoder and Decoder:**  
    يتألف نموذج التحول من مشفر وفك شفرة، حيث يتألف كل من المشفر وفك الشفرة من عدة طبقات. كل طبقة تحتوي على طبقتين فرعيتين: آلية انتباه الذات متعددة الرؤوس وشبكة عصبية تغذية للأمام.
    
      - **_Encoder:_** المشفر يأخذ تسلسل الإدخال ويعالجه من خلال عدة طبقات من آليات الانتباه الذاتي وشبكات التغذية للأمام. يلتقط المشفر المعلومات السياقية لكل كلمة استنادًا إلى السلسلة بأكملها.
        
      - **_Decoder:_** الفك شفرة يولد تسلسل الإخراج كلمة بعد كلمة، حيث يراعي أجزاء التسلسل الداخلي المشفرة ذات الصلة. يتضمن أيضًا آلية انتباه إضافية تُسمى "انتباه المشفر-فك الشفرة" تساعد النموذج في التركيز على الإدخال أثناء عملية الفك شفرة.
    
4. **Self-Attention Mechanism:**   
	- __first what is self attention:__ يتمثل جوهر نموذج التحول في آلية الانتباه الذاتي. تسمح هذه الآلية لكل كلمة في تسلسل الإدخال بالانتباه إلى جميع الكلمات الأخرى، مما يتيح لها التقاط الصلة والتأثير. تعمل هذه الآلية عن طريق مراقبة مدى تشابه وأهمية كل كلمة مع جميع الكلمات في الجملة، بما في ذلك نفسها.
	- **ثانيًا الآلية:** 
		- **_Multi-head attention in the encoder block_:** تلعب دوراً حاسمًا في التقاط أنواع مختلفة من المعلومات وتعلم العلاقات المتنوعة بين الكلمات. تتيح للنموذج الانتباه الذاتي الانتباه إلى أجزاء مختلفة من تسلسل الإدخال في وقت واحد، وتعلم تمثيلات متعددة لنفس الإدخال.
		   ![[Pasted image 20231224165444.png]]
	
		- **_Masked Multi-head attention in the decoder block_:** تعتبر آلية الانتباه متعددة الرؤوس في كتلة المشفر هي نفسها كآلية الانتباه متعددة الرؤوس، ولكن هذه المرة تُستخدم للجملة المترجمة، بهدف ضمان أنه خلال عملية الفك شفرة، يمكن لكل كلمة أن تنتبه فقط إلى الكلمات التي تسبقها. يقوم هذا الترتيب بمنع النموذج من الوصول إلى المعلومات المستقبلية، وهو أمر أساسي لتوليد تسلسل الإخراج خطوة بخطوة.
		- _**Multi-head attention in the decoder block_:** تُطبق نفس آلية الانتباه متعددة الرؤوس كما في كتلة المشفر، ولكن هذه المرة بين جملة الإدخال وجملة الترجمة، بهدف التقاط علاقات متنوعة بين تسلسل الإدخال وتسلسل الإخراج الذي تم إنشاؤه. تسمح هذه الآلية للفك شفرة بالانتباه إلى أجزاء مختلفة من إخراج المشفر وتعلم تمثيلات متعددة للسياق.
	
5. **Feed Forward in two blocks:** هو ببساطة شبكة عصبية تغذية للأمام، ولكن في هذه الورقة يكون عدد الخلايا العصبية 2048. 

6. **Add & Normalization.** 
    $$Z^{(i)}_{\text{norm}} = Z^{(i)} - \frac{\mu}{\sqrt{\sigma^2 + \epsilon}}$$
    اختياري باستخدام معلمات قابلة للتعلم:$$Z^{\tilde{(i)}} = \gamma Z_{\text{norm}}^{(i)} + \beta$$
 ![[TNN.drawio 1.png]]
---

### آلية الانتباه الذاتي:

جوهر نموذج التحول هو آلية الانتباه الذاتي. تتيح هذه الآلية لكل كلمة في تسلسل الإدخال أن تنتبه إلى جميع الكلمات الأخرى، مما يتيح لها التقاط الصلة والتأثير. تحسب آلية الانتباه الذاتي ثلاثة متجهات لكل كلمة: الاستعلام (Query)، والمفتاح (Key)، والقيمة (Value).

- الاستعلام (Q): تعمل كل كلمة كاستعلام لحساب درجات الانتباه.
	- Q: ما الذي أبحث عنه.
- المفتاح (K): تعمل كل كلمة كمفتاح لتحديد صلتها بالكلمات الأخرى.
	- K: ما الذي يمكنني تقديمه.
- القيمة (V): تساهم كل كلمة كقيمة في المجموع المرتب طبقاً لوزنات الانتباه.
	- ما الذي أقدمه فعليًا.
   ![[Pasted image 20231224165535.png]]
   ![[Pasted image 20231224165611.png]]
---

 ![[Pasted image 20231221101118.png]]
التسلسل الانتباهي لكل كلمة باستخدام هذه الصيغة: 
$$Z = \text{softmax}\left(\frac{QK^T}{\sqrt{\text{Dimension of vector } Q, K \text{ or } V}}\right)V$$
يتم حساب الانتباه الذاتي عن طريق جمع المنتج النقطي بين الاستعلام والمفتاح، مع تقديم توسيع بمعامل، ثم تطبيق وظيفة softmax للحصول على أوزان الانتباه. تحدد هذه الأوزان أهمية قيمة كل كلمة للكلمة الحالية.
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
$$\text{self attention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_{\text{model}}}}+M\right)$$
### Output:

الطبقة النهائية للفك شفرة هي إسقاط ليني تليه وظيفة تنشيط softmax. ينتج عنها توزيع احتمالي على المفردات، مما يسمح للنموذج بإنشاء كلمة الإخراج عن طريق أخذ عينات من هذا التوزيع.

### Softmax:

وظيفة softmax هي وظيفة رياضية تحوّل متجهًا من K أعداد حقيقية إلى توزيع احتمالات لـ K نتائج ممكنة. إنها تعد تعميمًا لوظيفة اللوجستية لعدة أبعاد، وتستخدم في الانحدار اللوجستي متعدد الفئات. وظيفة softmax غالبًا ما تُستخدم كوظيفة تنشيط آخر للشبكة العصبية لتوحيد إخراج الشبكة إلى توزيع احتمال على فئات الإخراج المتوقعة. صيغة الوظيفة القياسية (الوحدية) للsoftmax هي كما يلي:
$$\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}
$$
​
### Linear 
لتحويل التضمينات إلى كلمات مرة أخرى (**_لديها فقط أوزان ولا تحتوي على انحيازات._**):

---

### التدريب:
$$H(P^*|P) = -\sum P^*(i) \log(P(i))$$
![[Pasted image 20231224194743.png]]

---

![[Pasted image 20231224194831.png]]
![[Pasted image 20231224195014.png]]
![[Pasted image 20231224195035.png]]
![[Pasted image 20231224195056.png]]
